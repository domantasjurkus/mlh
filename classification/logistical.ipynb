{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00021787 0.04819778 0.10811003 0.01479095] 0\n",
      "[-0.15435782  3.3444323   6.05047574  2.27144526] 10000\n",
      "[-0.31994812  4.81570632  8.64652651  3.48190252] 20000\n",
      "[-0.48390063  5.83542163 10.5562751   4.39316969] 30000\n",
      "[-0.64689634  6.57307754 12.1027191   5.10490868] 40000\n",
      "[-0.80951603  7.10266592 13.40074479  5.67233209] 50000\n",
      "[-0.97221191  7.46873164 14.50966785  6.12839601] 60000\n",
      "[-1.13525673  7.70262355 15.4682123   6.49528543] 70000\n",
      "[-1.29878461  7.82816098 16.30498178  6.78924957] 80000\n",
      "[-1.46284005  7.8641594  17.04236976  7.02282214] 90000\n",
      "[-1.62741434  7.82579159 17.69837116  7.20598033] 100000\n",
      "[-1.79246944  7.72543601 18.28761221  7.34682669] 110000\n",
      "[-1.95795265  7.57326276 18.82203888  7.45203103] 120000\n",
      "[-2.12380545  7.37766653 19.31142718  7.52713787] 130000\n",
      "[-2.28996862  7.14560027 19.76378325  7.57679122] 140000\n",
      "[-2.45638509  6.88283864 20.18566559  7.60490464] 150000\n",
      "[-2.6230014   6.59418942 20.58244754  7.61479293] 160000\n",
      "[-2.78976844  6.28366448 20.95853193  7.60927585] 170000\n",
      "[-2.95664161  5.95461898 21.31752694  7.59076075] 180000\n",
      "[-3.12358077  5.60986506 21.66239002  7.5613092 ] 190000\n",
      "[-3.29055005  5.25176503 21.99554561  7.52269104] 200000\n",
      "[-3.45751762  4.88230784 22.31898147  7.47642869] 210000\n",
      "[-3.62445532  4.50317211 22.63432747  7.42383363] 220000\n",
      "[-3.79133843  4.11577795 22.94291998  7.36603672] 230000\n",
      "[-3.95814534  3.72132988 23.24585461  7.30401353] 240000\n",
      "[-4.1248573   3.32085227 23.54402934  7.23860564] 250000\n",
      "[-4.29145811  2.91521868 23.83817989  7.17053868] 260000\n",
      "[-4.45793394  2.50517631 24.12890871  7.10043769] 270000\n",
      "[-4.62427308  2.09136616 24.41670878  7.02884022] 280000\n",
      "[-4.7904657   1.67433995 24.70198314  6.95620764] 290000\n",
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "trainX = np.loadtxt('X_train.csv', delimiter=',', skiprows=1)\n",
    "traint = np.loadtxt('y_train.csv', delimiter=',', skiprows=1)\n",
    "traint = traint[:,1][:,None]-1\n",
    "testX = np.loadtxt('X_test.csv', delimiter=',', skiprows=1)\n",
    "    \n",
    "def to_binary(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def h(x, w):\n",
    "    return sigmoid(np.dot(x, w))\n",
    "\n",
    "def sigmoid(x):\n",
    "    val = scipy.special.expit(x)\n",
    "    return val\n",
    "\n",
    "def cv(trainX, traint, folds=20, regularFactor=0):\n",
    "    sample_count = trainX.shape[0]\n",
    "    feature_count = trainX.shape[1]\n",
    "    fold_size = sample_count//folds\n",
    "    \n",
    "    average_accuracy = 0\n",
    "    \n",
    "    for i in range(folds):\n",
    "        split_index1 = i*fold_size\n",
    "        split_index2 = (i+1)*fold_size\n",
    "        \n",
    "        cv_testX = trainX[split_index1:split_index2]\n",
    "        cv_testt = traint[split_index1:split_index2]\n",
    "        cv_trainX = np.concatenate((trainX[:split_index1], trainX[split_index2:]))\n",
    "        cv_traint = np.concatenate((traint[:split_index1], traint[split_index2:]))\n",
    "              \n",
    "        w = get_gd_weights(cv_trainX, cv_traint, 0.9, regularFactor)\n",
    "        predictions = np.array(list(map(to_binary, h(cv_testX, w))))[:,None]\n",
    "        \n",
    "        #\n",
    "        # Sklearn models\n",
    "        #\n",
    "        #model = get_sk_model(cv_trainX, cv_traint)\n",
    "        #predictions = model.predict(cv_testX)\n",
    "        #print(predictions[0:5])\n",
    "        #print(cv_testt[0:5])\n",
    "        \n",
    "        acc = get_accuracy(predictions, cv_testt)\n",
    "        average_accuracy += acc\n",
    "        #print(\"Iteration and accuracy:\", i, acc)\n",
    "    \n",
    "    #print(\"Total accuracy:\", average_accuracy/folds)\n",
    "    return average_accuracy/folds\n",
    "    \n",
    "def get_accuracy(predictions, actual):\n",
    "    N = predictions.shape[0]\n",
    "    temp = predictions-actual\n",
    "    misses = np.count_nonzero(temp)\n",
    "    hits = N-misses\n",
    "    return hits/N\n",
    "        \n",
    "def make_predictions(X, w):\n",
    "    preds = np.zeros((X.shape[0], 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        preds[i] = to_binary(h(x, w))\n",
    "    return preds\n",
    "\n",
    "def filter_by_variance(trainX, keep_count=10):\n",
    "    sample_count = trainX.shape[0]\n",
    "    feature_count = trainX.shape[1]\n",
    "    variances = np.zeros(feature_count)\n",
    "    \n",
    "    # Find variance of each feature\n",
    "    for i in range(feature_count):\n",
    "        feature = trainX.T[i]\n",
    "        variance = np.var(feature)\n",
    "        variances[i] = variance    \n",
    "    \n",
    "    features_by_variance = variances.argsort()[:keep_count]\n",
    "\n",
    "    newX = np.zeros((sample_count, keep_count))\n",
    "    for i in range(keep_count):\n",
    "        newX[:,i] = trainX[:,features_by_variance[i]]\n",
    "\n",
    "    return newX, features_by_variance\n",
    "\n",
    "\n",
    "\n",
    "# Newton Raphson\n",
    "\"\"\"def get_nr_weights(trainx, traint, iterations=20):\n",
    "    n_weights = trainx.shape[1]\n",
    "    \n",
    "    w = np.zeros((n_weights,1))\n",
    "    dx = np.diag(np.dot(trainx, trainx.T))[:,None]\n",
    "    \n",
    "    # allw - for debugging\n",
    "    #allw = np.zeros((n_weights,iterations))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        #allw[:,i] = w.flatten()\n",
    "        P = 1.0/(1.0 + np.exp(-np.dot(trainx, w)))\n",
    "        gw = -w + np.sum(trainx*np.tile(traint-P,(1,n_weights)), axis=0)[:,None]\n",
    "        temp = trainx*np.tile(P*(1-P), (1,n_weights))\n",
    "        hw = -np.eye(n_weights) - np.dot(temp.T, trainx)\n",
    "        w = w - np.dot(np.linalg.inv(hw), gw)\n",
    "    \n",
    "    return w\"\"\"\n",
    "\n",
    "# Gradient descent\n",
    "def get_gd_weights(X, t, iterations=5000, alpha=0.9, regularFactor=0):\n",
    "    sample_count = X.shape[0]\n",
    "    weight_count = X.shape[1]\n",
    "    w = np.zeros(weight_count)[:,None]\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        w = w*(1-(alpha*regularFactor)/sample_count) - alpha/sample_count * np.dot(X.T, h(X,w)-t)\n",
    "        \n",
    "        # Check if gradient descent is running\n",
    "        if i%10000 == 0: print(w[0:4].ravel(), i)\n",
    "\n",
    "    return w\n",
    "\n",
    "# 22 is chosen as a heuristic\n",
    "trainX, top_feature_indices = filter_by_variance(trainX, 22)\n",
    "\n",
    "#w = get_nr_weights(trainX, traint)\n",
    "w = get_gd_weights(trainX, traint, 300000, 0.9)\n",
    "\n",
    "# Single CV\n",
    "#cv(trainX, traint)\n",
    "\n",
    "# Loop CV to find subset of features\n",
    "# for feature_count in range(1, 112):\n",
    "#     newX, _ = filter_by_ttest(trainX, traint, feature_count)\n",
    "#     #newX, _ = filter_by_variance(trainX, feature_count)\n",
    "#     cv(newX, traint)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_predictions(predictions, filename=\"predictions_logit.csv\"):\n",
    "    N = predictions.shape[0]\n",
    "    output = np.ones((N, 2))\n",
    "    output[:,0] = range(N)\n",
    "    output[:,1] = predictions.ravel()\n",
    "    np.savetxt(filename, output, fmt='%d', delimiter=\",\", header=\"Id,EpiOrStroma\")\n",
    "    print(\"Predictions saved\")\n",
    "\n",
    "# Select only top features if filtered\n",
    "testX = testX[:,top_feature_indices]\n",
    "preds = make_predictions(testX, w)\n",
    "save_predictions(preds+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
