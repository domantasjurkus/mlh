{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#EPSILON=0.5\n",
    "\n",
    "# Datum\n",
    "trainX = np.loadtxt('X_train.csv', delimiter=',', skiprows=1)\n",
    "traint = np.loadtxt('y_train.csv', delimiter=',', skiprows=1)\n",
    "traint = traint[:,1][:,None]-1\n",
    "\n",
    "testX = np.loadtxt('X_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "\"\"\"def plot_pair(X, t, f1, f2):\n",
    "    plt.figure()\n",
    "    pos0 = np.where(t==0)[0]\n",
    "    pos1 = np.where(t==1)[0]\n",
    "    plt.plot(X[pos1,f1], X[pos1,f2],'bo')\n",
    "    plt.plot(X[pos0,f1], X[pos0,f2],'ro')\"\"\"\n",
    "\n",
    "def save_predictions(predictions, filename=\"class_logit.csv\"):\n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    output = np.ones((N, 2))\n",
    "    output[:,0] = range(N)\n",
    "    output[:,1] = predictions.ravel()\n",
    "    np.savetxt(filename, output, fmt='%d', delimiter=\",\", header=\"Id,EpiOrStroma\")\n",
    "    \n",
    "def to_binary(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def h(x, w):\n",
    "    return sigmoid(np.dot(x, w))\n",
    "\n",
    "def sigmoid(x):\n",
    "    val = scipy.special.expit(x)\n",
    "    return val\n",
    "\n",
    "def cv(trainX, traint, folds=20):\n",
    "    sample_count = trainX.shape[0]\n",
    "    fold_size = sample_count//folds\n",
    "    \n",
    "    average_accuracy = 0\n",
    "    \n",
    "    for i in range(folds):\n",
    "        split_index1 = i*fold_size\n",
    "        split_index2 = (i+1)*fold_size\n",
    "        \n",
    "        cv_testX = trainX[split_index1:split_index2]\n",
    "        cv_testt = traint[split_index1:split_index2]\n",
    "        cv_trainX = np.concatenate((trainX[:split_index1], trainX[split_index2:]))\n",
    "        cv_traint = np.concatenate((traint[:split_index1], traint[split_index2:]))\n",
    "        \n",
    "        #\n",
    "        # Pick model here\n",
    "        #        \n",
    "        #w = get_nr_weights(cv_trainX, cv_traint)\n",
    "        #predictions = make_predictions(cv_testX, w)\n",
    "        model = get_sk_model(cv_trainX, cv_traint)\n",
    "        predictions = model.predict(cv_testX)\n",
    "        #print(predictions[0:5])\n",
    "        #print(cv_testt[0:5])\n",
    "        \n",
    "        acc = get_accuracy(predictions[:,None], cv_testt)\n",
    "        average_accuracy += acc\n",
    "        #print(\"Iteration and accuracy:\", i, acc)\n",
    "    \n",
    "    print(\"Total accuracy:\", average_accuracy/folds)\n",
    "    \n",
    "def get_accuracy(predictions, actual):\n",
    "    N = predictions.shape[0]\n",
    "    temp = predictions-actual\n",
    "    misses = np.count_nonzero(temp)\n",
    "    hits = N-misses\n",
    "    return hits/N\n",
    "        \n",
    "def make_predictions(X, w):\n",
    "    preds = np.zeros((X.shape[0], 1))\n",
    "    for i in range(X.shape[0]):\n",
    "        x = X[i]\n",
    "        preds[i] = to_binary(h(x, w))\n",
    "    return preds\n",
    "        \n",
    "def filter_by_ttest(trainX, traint, n=10):\n",
    "    ps = np.zeros(trainX.shape[1])\n",
    "    \n",
    "    for i in range(trainX.shape[1]):\n",
    "        feature = trainX.T[i][:,None]\n",
    "        label = traint.flatten()[:,None]\n",
    "        \n",
    "        stacked = np.hstack((feature, label))\n",
    "        stacked.sort(axis=0)\n",
    "        count = (stacked[:,1] == 1).sum()\n",
    "        group1 = stacked[:count,0]\n",
    "        group2 = stacked[count:,0]\n",
    "        \n",
    "        p = stats.ttest_ind(group1,group2)[1]\n",
    "        ps[i] = p\n",
    "        \n",
    "    features_by_distinction = ps.argsort()[:n]\n",
    "    #print(features_by_distinction)\n",
    "    newX = np.zeros((trainX.shape[0], n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        newX[:,i] = trainX[:,features_by_distinction[i]]\n",
    "    \n",
    "    return newX, features_by_distinction\n",
    "\n",
    "def filter_by_variance(trainX, keep_count=10):\n",
    "    sample_count = trainX.shape[0]\n",
    "    feature_count = trainX.shape[1]\n",
    "    variances = np.zeros(feature_count)\n",
    "    \n",
    "    # Find variance of each feature\n",
    "    for i in range(feature_count):\n",
    "        feature = trainX.T[i]\n",
    "        variance = np.var(feature)\n",
    "        variances[i] = variance    \n",
    "    \n",
    "    features_by_variance = variances.argsort()[:keep_count]\n",
    "\n",
    "    newX = np.zeros((sample_count, keep_count))\n",
    "    for i in range(keep_count):\n",
    "        newX[:,i] = trainX[:,features_by_variance[i]]\n",
    "\n",
    "    return newX, features_by_variance\n",
    "\n",
    "def get_sk_model(trainX, traint):\n",
    "    clf = LogisticRegression()\n",
    "    model = clf.fit(trainX, traint.ravel())\n",
    "    return model\n",
    "\n",
    "\"\"\"def predict_sklearn(model, testX):\n",
    "    predictions = model.predict(testX)\n",
    "    return predictions\"\"\"\n",
    "\n",
    "def get_nr_weights(trainx, traint, iterations=20):\n",
    "    n_weights = trainx.shape[1]\n",
    "    \n",
    "    w = np.zeros((n_weights,1))\n",
    "    dx = np.diag(np.dot(trainx, trainx.T))[:,None]\n",
    "    \n",
    "    # For debugging\n",
    "    allw = np.zeros((n_weights,iterations))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        allw[:,i] = w.flatten()\n",
    "        P = 1.0/(1.0 + np.exp(-np.dot(trainx, w)))\n",
    "        gw = -w + np.sum(trainx*np.tile(traint-P,(1,n_weights)), axis=0)[:,None]\n",
    "        temp = trainx*np.tile(P*(1-P), (1,n_weights))\n",
    "        hw = -np.eye(n_weights) - np.dot(temp.T, trainx)\n",
    "        w = w - np.dot(np.linalg.inv(hw), gw)\n",
    "        \n",
    "        #print(w[0:10])\n",
    "    \n",
    "    # For debugging\n",
    "#     [plt.plot(allw[i,:]) for i in range(100)]\n",
    "#     plt.xlabel('Iteration')\n",
    "#     plt.ylabel('w')\n",
    "    \n",
    "    return w\n",
    "\n",
    "def get_gd_weights(X, t, iterations=500, alpha=0.1):\n",
    "    # Gradient descent (really slow and inaccurate)\n",
    "    w = np.zeros(trainX.shape[1])[:,None]\n",
    "    sample_count = X.shape[0]\n",
    "    weight_count = X.shape[1]\n",
    "    \n",
    "    for iters in range(predictionsiterations):\n",
    "        for j in range(weight_count):\n",
    "            change_sum = 0\n",
    "            for i in range(sample_count):\n",
    "                prediction = h(X[i,:], w)\n",
    "                change = (prediction-t[i])*X[i,j]\n",
    "                change_sum += change\n",
    "                \n",
    "            w[j] = w[j] - alpha*change_sum\n",
    "        \n",
    "        alpha *= 0.99\n",
    "        print(w[0:3].T, alpha)\n",
    "        #print(\"Updated weights after iteration\", iters)\n",
    "        \n",
    "    return w\n",
    "\n",
    "# Filter by ttest\n",
    "#trainX, top_feature_indices = filter_by_ttest(trainX, traint, 21)\n",
    "trainX, top_feature_indices = filter_by_variance(trainX, 25)\n",
    "\n",
    "# Attach feature column for bias term (?)\n",
    "#trainX = np.hstack((np.ones_like(trainX[:,0][:,None]), trainX))\n",
    "\n",
    "#w = get_gd_weights(w, trainX, traint)\n",
    "#w = get_nr_weights(trainX, traint, 20)\n",
    "\n",
    "# Single CV\n",
    "#cv(trainX, traint)\n",
    "\n",
    "# Loop CV\n",
    "# for feature_count in range(1, 112):\n",
    "#     #newX, _ = filter_by_ttest(trainX, traint, feature_count)\n",
    "#     newX, _ = filter_by_variance(trainX, feature_count)\n",
    "#     cv(newX, traint)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_predictions(predictions, filename=\"predictions.csv\"):\n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    output = np.ones((N, 2))\n",
    "    output[:,0] = range(N)\n",
    "    output[:,1] = predictions.ravel()\n",
    "    np.savetxt(filename, output, fmt='%d', delimiter=\",\", header=\"Id,EpiOrStroma\")\n",
    "    print(\"Predictions saved\")\n",
    "\n",
    "# Save predictions\n",
    "\n",
    "# Select only top features if filtered\n",
    "testX = testX[:,top_feature_indices]\n",
    "\n",
    "#preds = make_predictions(testX, w)\n",
    "model = get_sk_model(trainX, traint)\n",
    "preds = model.predict(testX)\n",
    "save_predictions(preds+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
