{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01071876  0.00779731  0.00382983]]\n",
      "[[-0.00527911  0.01703129  0.01807587]]\n",
      "[[-0.01571354  0.01140752  0.01151097]]\n",
      "[[-0.0264323   0.0036244   0.00171995]]\n",
      "[[-0.02533097  0.00258822  0.00165868]]\n",
      "[[-0.02348141  0.00257601  0.00258297]]\n",
      "[[-0.02136136  0.00282812  0.00386132]]\n",
      "[[-0.01736551  0.00702811  0.00980904]]\n",
      "[[-0.01521939  0.01443428  0.02132136]]\n",
      "[[-0.02575261  0.00782329  0.01334688]]\n",
      "[[-3.64713741e-02  4.01763000e-05  3.55586830e-03]]\n",
      "[[-0.03533708 -0.00093292  0.00357121]]\n",
      "[[-0.03391232 -0.00143447  0.00399034]]\n",
      "[[-0.03207168 -0.00146456  0.00485982]]\n",
      "[[-0.02979346 -0.00082853  0.00648995]]\n",
      "[[-0.02606049  0.00382506  0.01335159]]\n",
      "[[-0.02422725  0.0110447   0.02463849]]\n",
      "[[-0.03476047  0.00442794  0.01662889]]\n",
      "[[-0.04547923 -0.00335517  0.00683787]]\n",
      "[[-0.04405448 -0.00385673  0.007257  ]]\n",
      "[[-0.04250623 -0.00419326  0.00783619]]\n",
      "[[-0.04063908 -0.00417169  0.0087822 ]]\n",
      "[[-0.03851561 -0.00291841  0.01092252]]\n",
      "[[-0.03517455  0.00251455  0.01957859]]\n",
      "[[-0.03469816  0.00898457  0.02962903]]\n",
      "[[-0.04532298  0.00177431  0.02049452]]\n",
      "[[-0.05591095 -0.00598939  0.01079201]]\n",
      "[[-0.05437174 -0.00638749  0.01133311]]\n",
      "[[-0.05282349 -0.00672402  0.0119123 ]]\n",
      "[[-0.0511623  -0.00683493  0.01268745]]\n",
      "[[-0.04937563 -0.00641373  0.01393617]]\n",
      "[[-0.04812957 -0.00599555  0.01503982]]\n",
      "[[-0.05361136 -0.0106516   0.00952875]]\n",
      "[[-0.05326742 -0.01213473  0.00865681]]\n",
      "[[-0.05124965 -0.01214736  0.00975432]]\n",
      "[[-0.04795748 -0.01051782  0.01262345]]\n",
      "[[-0.04184658 -0.00081273  0.02745954]]\n",
      "[[-0.04953353 -0.00128656  0.02818561]]\n",
      "[[-0.0602523  -0.00906968  0.0183946 ]]\n",
      "[[-0.05847774 -0.00858006  0.0197468 ]]\n",
      "[[-0.0581277  -0.00861901  0.02026545]]\n",
      "[[-0.06400177 -0.01349797  0.01455232]]\n",
      "[[-0.06325266 -0.0147606   0.01412614]]\n",
      "[[-0.06139884 -0.01484548  0.01501757]]\n",
      "[[-0.05918397 -0.01447071  0.01642909]]\n",
      "[[-0.05476209 -0.00939505  0.02372115]]\n",
      "[[-0.05582945 -0.00390718  0.03263021]]\n",
      "[[-0.0663234  -0.01042135  0.02476966]]\n",
      "[[-0.07704217 -0.01820446  0.01497865]]\n",
      "[[-0.07621712 -0.01940056  0.01463484]]\n",
      "[[-0.07480696 -0.01994802  0.01501673]]\n",
      "[[-0.07327195 -0.02033337  0.01555791]]\n",
      "[[-0.07152297 -0.02051737  0.01630491]]\n",
      "[[-0.06887907 -0.01930363  0.01862032]]\n",
      "[[-0.06289175 -0.00960119  0.03337236]]\n",
      "[[-0.06863894 -0.00781585  0.03703856]]\n",
      "[[-0.07935771 -0.01559897  0.02724755]]\n",
      "[[-0.081863   -0.01711304  0.02566748]]\n",
      "[[-0.08682786 -0.02147581  0.02077529]]\n",
      "[[-0.08551157 -0.02216379  0.02106427]]\n",
      "[[-0.08380045 -0.02226573  0.02187876]]\n",
      "[[-0.08202284 -0.02184558  0.02308502]]\n",
      "[[-0.08067766 -0.02102918  0.02453917]]\n",
      "[[-0.08746744 -0.02637068  0.01800925]]\n",
      "[[-0.08697219 -0.02773687  0.01727389]]\n",
      "[[-0.08466241 -0.02747911  0.01866445]]\n",
      "[[-0.08142251 -0.02595785  0.02145215]]\n",
      "[[-0.0753116  -0.01625276  0.03628824]]\n",
      "[[-0.0848653  -0.01981064  0.03245564]]\n",
      "[[-0.09558406 -0.02759376  0.02266463]]\n",
      "[[-0.09475901 -0.02878986  0.02232082]]\n",
      "[[-0.0932583  -0.02926095  0.02278559]]\n",
      "[[-0.09154451 -0.02949541  0.02350258]]\n",
      "[[-0.08929344 -0.02903256  0.02498882]]\n",
      "[[-0.08389063 -0.02047291  0.03796906]]\n",
      "[[-0.08745931 -0.0170355   0.0438163 ]]\n",
      "[[-0.09813794 -0.0247121   0.03417268]]\n",
      "[[-0.10838492 -0.03228492  0.0247003 ]]\n",
      "[[-0.10696017 -0.03278647  0.02511943]]\n",
      "[[-0.10553541 -0.03328803  0.02553855]]\n",
      "[[-0.1039962  -0.03368613  0.02607965]]\n",
      "[[-0.1022417  -0.03376283  0.02690282]]\n",
      "[[-0.09999081 -0.03295621  0.0286391 ]]\n",
      "[[-0.09611758 -0.02755821  0.03688457]]\n",
      "[[-0.09541801 -0.02099771  0.04715381]]\n",
      "[[-0.10598167 -0.02792158  0.03849731]]\n",
      "[[-0.11670043 -0.0357047   0.0287063 ]]\n",
      "[[-0.11527567 -0.03620625  0.02912542]]\n",
      "[[-0.11372743 -0.03654278  0.02970461]]\n",
      "[[-0.11206624 -0.0366537   0.03047976]]\n",
      "[[-0.11013701 -0.0361465   0.03186044]]\n",
      "[[-0.1083984  -0.03459897  0.0342513 ]]\n",
      "[[-0.11163829 -0.0363776   0.0323114 ]]\n",
      "[[-0.12230844 -0.04407597  0.02263104]]\n",
      "[[-0.12122793 -0.04490659  0.02256072]]\n",
      "[[-0.11925313 -0.04501648  0.02348759]]\n",
      "[[-0.11698162 -0.04489976  0.02474271]]\n",
      "[[-0.11421124 -0.04410243  0.02670274]]\n",
      "[[-0.10906761 -0.03921845  0.0338359 ]]\n",
      "[[-0.11014634 -0.03361166  0.04297804]]\n",
      "[[-0.11856001 -0.03481168  0.04254134]]\n",
      "[[-0.12927878 -0.0425948   0.03275033]]\n",
      "[[-0.12723369 -0.04153452  0.03462766]]\n",
      "[[-0.12635431 -0.04062886  0.03617857]]\n",
      "[[-0.13134083 -0.04486853  0.03147364]]\n",
      "[[-0.13406594 -0.04818418  0.02797278]]\n",
      "[[-0.13221625 -0.04833903  0.02882676]]\n",
      "[[-0.1302135  -0.04827645  0.02988252]]\n",
      "[[-0.12769906 -0.04753731  0.03173474]]\n",
      "[[-0.12171173 -0.03783487  0.04648679]]\n",
      "[[-0.12648644 -0.03505402  0.05132174]]\n",
      "[[-0.1372052  -0.04283714  0.04153072]]\n",
      "[[-0.14350819 -0.04779196  0.03569452]]\n",
      "[[-0.14490361 -0.05009303  0.03343735]]\n",
      "[[-0.14334471 -0.0503685   0.03405579]]\n",
      "[[-0.14168353 -0.05047941  0.03483093]]\n",
      "[[-0.13990592 -0.05005926  0.03603719]]\n",
      "[[-0.13848831 -0.04885199  0.03791021]]\n",
      "[[-0.14555869 -0.05434274  0.03117599]]\n",
      "[[-0.14609665 -0.05641977  0.02945697]]\n",
      "[[-0.14394194 -0.05633402  0.0306145 ]]\n",
      "[[-0.14081165 -0.05506673  0.03304759]]\n",
      "[[-0.13470074 -0.04536163  0.04788368]]\n",
      "[[-0.13829137 -0.04134322  0.05448511]]\n",
      "[[-0.14901013 -0.04912634  0.0446941 ]]\n",
      "[[-0.15832764 -0.05591284  0.03615819]]\n",
      "[[-0.15807996 -0.05740064  0.03531557]]\n",
      "[[-0.15654074 -0.05779874  0.03585666]]\n",
      "[[-0.15488569 -0.05792472  0.03659459]]\n",
      "[[-0.15286131 -0.05732746  0.03802338]]\n",
      "[[-0.15107664 -0.05586361  0.04033553]]\n",
      "[[-0.15493364 -0.05894841  0.03693804]]\n",
      "[[-0.16495214 -0.06621007  0.02778128]]\n",
      "[[-0.16347667 -0.06678162  0.02809412]]\n",
      "[[-0.1615086  -0.06688189  0.02898574]]\n",
      "[[-0.15937421 -0.0668466   0.03008184]]\n",
      "[[-0.15677425 -0.06622824  0.03180697]]\n",
      "[[-0.15190366 -0.06227055  0.03784567]]\n",
      "[[-0.14882266 -0.05439227  0.05013772]]\n",
      "[[-0.15647018 -0.05399628  0.05193834]]\n",
      "[[-0.16718894 -0.0617794   0.04214732]]\n",
      "[[-0.16402922 -0.05767807  0.0481313 ]]\n",
      "[[-0.1632465  -0.05219262  0.05652649]]\n",
      "[[-0.17255286 -0.05710692  0.05045821]]\n",
      "[[-0.18317163 -0.06488485  0.04071378]]\n",
      "[[-0.18203126 -0.06586044  0.040728  ]]\n",
      "[[-0.18060521 -0.0663588   0.04114606]]\n",
      "[[-0.17909582 -0.06667457  0.0417186 ]]\n",
      "[[-0.17751307 -0.06679535  0.04244292]]\n",
      "[[-0.17596008 -0.06656821  0.04335402]]\n",
      "[[-0.17474785 -0.06623801  0.04437885]]\n",
      "[[-0.18049858 -0.07108432  0.03860701]]\n",
      "[[-0.17997061 -0.07248655  0.03790291]]\n",
      "[[-0.17761545 -0.07214281  0.03938083]]\n",
      "[[-0.17432689 -0.07041527  0.04232209]]\n",
      "[[-0.16821598 -0.06071017  0.05715818]]\n",
      "[[-0.17779785 -0.06442416  0.05313861]]\n",
      "[[-0.18851661 -0.07220728  0.0433476 ]]\n",
      "[[-0.18764927 -0.07338472  0.04304089]]\n",
      "[[-0.1861839  -0.07381888  0.04350356]]\n",
      "[[-0.18438844 -0.07396451  0.04430316]]\n",
      "[[-0.18211845 -0.07339975  0.04582903]]\n",
      "[[-0.17684066 -0.06553661  0.05766205]]\n",
      "[[-0.17967649 -0.06150984  0.06442938]]\n",
      "[[-0.19031075 -0.06885372  0.05504427]]\n",
      "[[-0.20092951 -0.07663165  0.04529983]]\n",
      "[[-0.19951936 -0.0771791   0.04568172]]\n",
      "[[-0.19809461 -0.07768066  0.04610084]]\n",
      "[[-0.19659279 -0.07810724  0.04660219]]\n",
      "[[-0.19490859 -0.07822951  0.04734672]]\n",
      "[[-0.19273058 -0.07773834  0.04877303]]\n",
      "[[-0.18963139 -0.07457482  0.05320131]]\n",
      "[[-0.18462822 -0.06556369  0.06712345]]\n",
      "[[-0.19521405 -0.07259427  0.05835565]]\n",
      "[[-0.20593282 -0.08037739  0.04856464]]\n",
      "[[-0.20476045 -0.08130968  0.04858497]]\n",
      "[[-0.20333569 -0.08181124  0.0490041 ]]\n",
      "[[-0.20159575 -0.0820568   0.04971301]]\n",
      "[[-0.19943378 -0.08175341  0.05103463]]\n",
      "[[-0.19498431 -0.07558634  0.06031825]]\n",
      "[[-0.19551737 -0.06963579  0.06959795]]\n",
      "[[-0.20610846 -0.07673979  0.06068378]]\n",
      "[[-0.21682723 -0.0845229   0.05089277]]\n",
      "[[-0.21564985 -0.0854272   0.0509505 ]]\n",
      "[[-0.21423825 -0.08596     0.05133043]]\n",
      "[[-0.21273643 -0.08638659  0.05183177]]\n",
      "[[-0.21098634 -0.08649303  0.05261837]]\n",
      "[[-0.20868184 -0.08572128  0.0543277 ]]\n",
      "[[-0.2049518  -0.08090096  0.06149534]]\n",
      "[[-0.20389756 -0.07406084  0.07215668]]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#EPSILON=0.5\n",
    "\n",
    "# Datum\n",
    "trainX = np.loadtxt('X_train.csv', delimiter=',', skiprows=1)\n",
    "traint = np.loadtxt('y_train.csv', delimiter=',', skiprows=1)\n",
    "traint = traint[:,1][:,None]-1\n",
    "testX = np.loadtxt('X_test.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "\"\"\"def plot_pair(X, t, f1, f2):\n",
    "    plt.figure()\n",
    "    pos0 = np.where(t==0)[0]\n",
    "    pos1 = np.where(t==1)[0]\n",
    "    plt.plot(X[pos1,f1], X[pos1,f2],'bo')\n",
    "    plt.plot(X[pos0,f1], X[pos0,f2],'ro')\"\"\"\n",
    "\n",
    "def save_predictions(predictions, filename=\"class_logit.csv\"):\n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    output = np.ones((N, 2))\n",
    "    output[:,0] = range(N)\n",
    "    output[:,1] = predictions.ravel()\n",
    "    np.savetxt(filename, output, fmt='%d', delimiter=\",\", header=\"Id,EpiOrStroma\")\n",
    "    \n",
    "def to_binary(x):\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "def h(x, w):\n",
    "    return sigmoid(np.dot(x, w))\n",
    "\n",
    "def sigmoid(x):\n",
    "    val = scipy.special.expit(x)\n",
    "    return val\n",
    "\n",
    "def filter_by_ttest(trainX, traint, n=10):\n",
    "    ps = np.zeros(trainX.shape[1])\n",
    "    \n",
    "    for i in range(trainX.shape[1]):\n",
    "        feature = trainX.T[i][:,None]\n",
    "        label = traint.flatten()[:,None]\n",
    "        \n",
    "        stacked = np.hstack((feature, label))\n",
    "        stacked.sort(axis=0)\n",
    "        count = (stacked[:,1] == 1).sum()\n",
    "        group1 = stacked[:count,0]\n",
    "        group2 = stacked[count:,0]\n",
    "        \n",
    "        p = stats.ttest_ind(group1,group2)[1]\n",
    "        ps[i] = p\n",
    "        \n",
    "    features_by_distinction = ps.argsort()[:n]\n",
    "    #print(features_by_distinction)\n",
    "    newX = np.zeros((trainX.shape[0], n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        newX[:,i] = trainX[:,features_by_distinction[i]]\n",
    "    \n",
    "    return newX, features_by_distinction\n",
    "\n",
    "# Results in worse performance\n",
    "\"\"\"def filter_by_variance(trainX, keep_count=10):\n",
    "    sample_count = trainX.shape[0]\n",
    "    feature_count = trainX.shape[1]\n",
    "    variances = np.zeros(feature_count)\n",
    "    \n",
    "    # Find variance of each feature\n",
    "    for i in range(feature_count):\n",
    "        feature = trainX.T[i]\n",
    "        variance = np.var(feature)\n",
    "        variances[i] = variance    \n",
    "    \n",
    "    features_by_variance = variances.argsort()[feature_count-keep_count:]\n",
    "\n",
    "    newX = np.zeros((sample_count, keep_count))\n",
    "    for i in range(keep_count):\n",
    "        newX[:,i] = trainX[:,features_by_variance[i]]\n",
    "\n",
    "    return newX, features_by_variance\"\"\"\n",
    "\n",
    "def get_optimal_weights(w, X, t, iterations=300, alpha=0.0001):\n",
    "    sample_count = X.shape[0]\n",
    "    weight_count = X.shape[1]\n",
    "    \n",
    "    for iters in range(iterations):\n",
    "        for j in range(weight_count):\n",
    "            change_sum = 0\n",
    "            for i in range(sample_count):\n",
    "                prediction = to_binary(h(X[i,:], w))\n",
    "                change = (prediction-t[i])*X[i,j]\n",
    "                change_sum += change\n",
    "                \n",
    "            w[j] = w[j] - alpha*change_sum\n",
    "        \n",
    "        print(w[0:3].T)\n",
    "        #print(\"Updated weights after iteration\", iters)\n",
    "        \n",
    "    return w\n",
    "\n",
    "# Variance filtering gives rubbish at the moment\n",
    "trainX, top_feature_indices = filter_by_ttest(trainX, traint, 10)\n",
    "#trainX, top_feature_indices = filter_by_variance(trainX, 60)\n",
    "\n",
    "w = np.zeros(trainX.shape[1])[:,None]\n",
    "w = get_optimal_weights(w, trainX, traint)\n",
    "\n",
    "# Save predictions\n",
    "testX = testX[:,top_feature_indices]\n",
    "\n",
    "def save_predictions(predictions, filename=\"predictions_sklearn.csv\"):\n",
    "    N = predictions.shape[0]\n",
    "    \n",
    "    output = np.ones((N, 2))\n",
    "    output[:,0] = range(N)\n",
    "    output[:,1] = predictions.ravel()\n",
    "    np.savetxt(filename, output, fmt='%d', delimiter=\",\", header=\"Id,EpiOrStroma\")\n",
    "    print(\"Predictions saved\")\n",
    "\n",
    "preds = np.zeros((testX.shape[0], 1))\n",
    "for i in range(testX.shape[0]):\n",
    "    preds[i] = to_binary(h(testX[i], w))\n",
    "save_predictions(preds+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
